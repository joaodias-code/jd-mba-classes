{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXECUTE ESSA CÉLULA PARA IMPORTAR AS BILBIOTECAS NECESSÁRIAS ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from funcoes_auxiliares import demonstracao_pca, DBSCAN_Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1)__ Os dados dos consumidores estão no arquivo `customers.csv`. Carregue os dados na variável `df` utilizando o método de pandas que lê arquivos csv e retorna um dataframe. Pesquise na documentação do Pandas se necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ### CARREGUE OS DADOS ###\n",
    "df.drop(['Regiao', 'Canal'], axis = 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2)__ Use o método `hist` de dataframes para gerar o histograma das variáveis para analisar a distribuição dos dados.\n",
    "\n",
    "Passe como argumento da função: `figsize=(15,10)` e `bins=15`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ### CHAME O HISTOGRAMA ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra visualização muito interessante é utilizando uma matriz de dispersão\n",
    "\n",
    "__3)__ Use a função `pandas.plotting.scatter_matrix` para gerar uma matriz contendo gráficos de dispersão entre as variáveis.\n",
    "\n",
    "Passe como argumento o dataframe,  `alpha=1`, `figsize=(17,10)` e `diagonal='kde'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ### CHAME A MATRIZ ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados seguem o que na literatura academica é bastante chamado de __skewed distribution__, como se fosse uma distribuição eviesada. Todas as variáveis tendem a essa distribuição e, dependendo da aplicação, isso pode não ser um problema, mas no nosso caso, onde iremos aplicar PCA e depois clusterização, é interessante aplicar uma __transformação logarítmica__ para distribuir os dados em uma distribuição normal.\n",
    "\n",
    "__4)__ Use a função `numpy.log` para fazer a log transformação de `df` e atribua a variável `df_log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = ### CHAME A FUNÇÃO ###\n",
    "df_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5)__ Use novamente o histograma e a scatter matrix para verificar a distribuição dos dados após a transformação logarítmica "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ### CHAME O HISTOGRAMA NOS DADOS DE LOG ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ### CHAME A MATRIZ NOS DADOS DE LOG ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "__6)__ Usando a função `demonstracao_pca` que plota os componentes principais extraídos com PCA, a variância explicada por cada componente e o peso de cada variável em cada componente, passe como argumento o dataframe normalizado e o número de componentes principais a serem plotados sendo igual a 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHAME A FUNÇÃO NOS DADOS TRANSFORMADOS ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7)__ Instancie o `PCA` com 2 componentes através do parâmetro `n_components`  e use a função `fit` passando o dataframe para extraír os 2 componentes principais.\n",
    "\n",
    "Após isso, utilize a função `transform` de `pca` passando `df_log` novamente. Será construído um novo dataframe contendo 2 dimensões (os dois componentes principais extraídos) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = ### INSTANCIE O PCA E USE O MÉTODO FIT NOS DADOS DE LOG ###\n",
    "dados_reduzidos = ### TRANSFORME OS DADOS USANDO O MÉTODO TRANSFORM ###\n",
    "dados_reduzidos = pd.DataFrame(dados_reduzidos, columns=['Dimensão 1', 'Dimensão 2'])\n",
    "graph = dados_reduzidos.plot.scatter(x='Dimensão 1', y='Dimensão 2', figsize=(10,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "__8)__ A função `DBSCAN_Clusters` recebe o dataframe reduzido, uma lista contendo valores para epsilon e um valor para  min_samples, e plota as clusterizações do DBSCAN para todas as combinações de hiperparâmetros. A função suporta plotar apenas 4 combinações, ou seja, é possível passar __por função__ 4 valores de epsilon e 1 de min_samples.\n",
    "\n",
    "Defina até 4 valores de epsilon além de uma lista contendo valores para min_sample e procure a combinação de valores que melhor segmenta os dados separando observações que você considera normais e as que você considera outliers.\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores_epsilon = ### DEFINA UMA LISTA COM ATÉ 4 VALORES DE EPSILON ###\n",
    "valores_min_sample = ### DEFINA UMA LISTA COM VALORES DE MIN_SAMPLE - QUANTO MAIS VALORES, MAIS TEMPO PROCESSANDO ###\n",
    "\n",
    "for min_sample in valores_min_sample:\n",
    "    DBSCAN_Clusters(dados_reduzidos, valores_epsilon, min_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__9)__ Usando a combinação mais apropriada de epsilon e min_samples, faça manualmente a clusterização utilizando esses parâmetros. O cluster de cada registro será acrescentado ao dataframe `dados_reduzidos` como uma nova coluna __status__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = clusters = ### INSTANCIE O MODELO PASSANDO OS PARÂMETROS ###.fit_predict(dados_reduzidos).fit_predict(dados_reduzidos)\n",
    "\n",
    "dados_reduzidos['status'] = ['Outlier' if cluster == -1 else 'Normal' for cluster in clusters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__10)__ Crie dois novos dataframes, `dataset_limpo` e `dataset_outliers`. O primeiro contem os registros considerados normais, e o segundo contem os registros considerados anômalos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_limpo = ### DADOS REDUZIDOS COM STATUS 'Normal' ###\n",
    "dataset_outliers = ### DADOS REDUZIDOS COM STATUS 'Outlier' ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXECUTE ESSA CÉLULA PARA PLOTAR A DISTRIBUIÇÃO ###\n",
    "graph = dataset_limpo.plot.scatter(\n",
    "          x = \"Dimensão 1\",\n",
    "          y = \"Dimensão 2\",\n",
    "          figsize=(6,3)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXECUTE ESSA CÉLULA PARA PLOTAR A DISTRIBUIÇÃO ###\n",
    "graph = dataset_outliers.plot.scatter(\n",
    "          x = \"Dimensão 1\",\n",
    "          y = \"Dimensão 2\",\n",
    "          figsize=(6,3)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__11)__ Quem são os outliers?\n",
    "\n",
    "Quando fazemos transformações nos dataframes, atrbuindo a novas variáveis versões transformadas de um dataframe original (`df` no nosso caso), o index dos registros se mantem inalterado a menos que usemos uma função tal como `DataFrame.reset_index()`. Ou seja, os registros tanto do dataset de valores normais quanto os registros do dataset de outliers ainda possuem o index do dataframe original. Exiba no dataframe original os registros que foram posteriormente classificados como outliers.\n",
    "\n",
    "DICA: podemos extrair o index dos registros de um dataframe simplesmente chamando o atributo `index` do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_original = ### REGISTROS DO DATAFRAME ORIGINAL CONSIDERADOS OUTLIERS ###\n",
    "outliers_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__12)__ Retorne um novo dataframe a partir de `outliers_original` contendo os z-scores desses registros (lembrando que o z-score é computado por coluna e não por registro, ou seja, o dataframe criado tem as mesmas dimensões do dataframe outliers_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe = ### CHAME A FUNÇÃO DESCRIBE() DO DATAFRAME ORIGINAL ###\n",
    "\n",
    "# Armazena as linhas 'std' e 'mean' do dataframe describe como arrays numpy, que suportam operações com vetores e álgebra linear\n",
    "std = np.round(### SELECIONE O REGISTRO DE 'std' DE describe ###, 2)\n",
    "mean = np.round(### SELECIONE O REGISTRO DE 'mean' DE describe ###, 2)\n",
    "\n",
    "### CRIE UMA FUNÇÃO computar_zscore QUE RECEBE x, mean e std, e retorna o Z-score ###\n",
    "\n",
    "# Constrói o dataframe com os z-scores\n",
    "# O conceito abaixo chama-se compreensão de dicionário, é semelhante a compreensão de lista\n",
    "pd.DataFrame({index:computar_zscore(outliers_original.loc[index], mean, std) for index in outliers_original.index}).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__13)__ RESPONDA: Se fossemos analisar esses dado sem o PCA e DBSCAN, usando uma métrica como o z-score em cada coluna, teriamos indentificado esses registros como outliers?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
